{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coursework 1\n",
    "\n",
    "# This notebook was used for the implementation and hyperparameter tuning purposes and it is not to be assumed as my final work!\n",
    "\n",
    "In this coursework you will be aiming to complete two classification tasks. One of the classification tasks is related to image classification and the other relates to text classification.\n",
    "\n",
    "The specific tasks and the marking for the various tasks are provided in the notebook. Each task is expected to be accompanied by a lab-report. Each task can have a concise lab report that is maximum of one page in an A4 size. You will be expected to submit your Jupyter Notebook and all lab reports as a single PDF file. You could have additional functions implemented that you require for carrying out each task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1\n",
    "\n",
    "In this task, you are provided with three classes of images, cars, bikes and people in real world settings. You are provided with code for obtaining features for these images (specifically histogram of gradients (HoG) features). You need to implement a boosting based classifier that can be used to classify the images. \n",
    "\n",
    "This task is worth 30 points out of 100 points. \n",
    "Implementing a working boosting based classifier and validating it by cross-validation on the training set will be evaluated for 15 out of 30 points. 10 points are based on the evaluation carried out on a separate test dataset that will be done at the time of evaluation. Finally 5 points are reserved for analysis of this part of the task and presenting it well in a lab report. \n",
    "\n",
    "Note that the boosting classifier you implement can include decision trees from your previous ML1 coursework or can be a decision stump. Use the image_dataset directory provided with the assignment and save it in the same directory as the Python notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write your  Image feature extraction code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "import scipy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_dataset(folder_name):\n",
    "    # assuming 128x128 size images and HoGDescriptor length of 34020\n",
    "    hog_feature_len=34020\n",
    "    hog = cv2.HOGDescriptor()\n",
    "    \n",
    "    X = np.empty([0,hog_feature_len], dtype=float)\n",
    "    ytargets = []\n",
    "    \n",
    "    directories_list = ['bikes', 'people', 'cars']\n",
    "    for directory in directories_list:\n",
    "        for filename in glob.glob(folder_name+'/'+directory+'/*.png'):\n",
    "            \n",
    "            im = cv2.imread(filename)\n",
    "            h = hog.compute(im).reshape(1,-1)\n",
    "\n",
    "            X = np.append(X, h, axis=0)\n",
    "            ytargets.append(directory)\n",
    "\n",
    "    y = np.array(ytargets)\n",
    "\n",
    "    return (X,y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boosting classifier class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoostingClassifier:\n",
    "\n",
    "    def __init__(self, num_classifiers):\n",
    "        \n",
    "        self.num_classifiers = num_classifiers\n",
    "        self.classifiers = []\n",
    "        self.classes = []\n",
    "    \n",
    "    def fit(self, X,y):\n",
    "        \n",
    "        n_samples, n_features = X.shape\n",
    "        self.classes = np.unique(y)\n",
    "        \n",
    "#         if len(self.classes) == 3:\n",
    "#             # M for images\n",
    "#             self.num_classifiers = 62\n",
    "#         elif len(self.classes) == 2:\n",
    "#             # M for text\n",
    "#             self.num_classifiers = 350\n",
    "        \n",
    "        # Initialize weights to 1/N\n",
    "        weights = np.full(n_samples, (1 / n_samples), dtype=np.float32)\n",
    "        \n",
    "        for classifier_index in range(self.num_classifiers):\n",
    "            classifier_tm = DecisionTreeClassifier(max_depth=1).fit(X,y,sample_weight=weights).predict\n",
    "            \n",
    "            I = np.where(classifier_tm(X) != y, 1, 0)\n",
    "            errM = sum(weights * I) / sum(weights)\n",
    "            \n",
    "            aM = np.log((1-errM)/errM) + np.log(len(self.classes)-1)\n",
    "        \n",
    "            for i in range(weights.shape[0]):\n",
    "                weights[i] = weights[i]*np.exp(aM*I[i])\n",
    "        \n",
    "            self.classifiers.append((aM, classifier_tm))\n",
    "        \n",
    "    \n",
    "    def predict(self, X):\n",
    "        classes = self.classes\n",
    "        \n",
    "        predictions = np.zeros([X.shape[0],1])\n",
    "        # Through each class\n",
    "        for clas in classes:\n",
    "            \n",
    "            class_sum = np.zeros([X.shape[0], 1])\n",
    "            \n",
    "            # Through each classifier\n",
    "            for classifier_index in range(self.num_classifiers):\n",
    "                \n",
    "                aM, classifier_tm = self.classifiers[classifier_index]\n",
    "\n",
    "                # To update sum\n",
    "                I = np.expand_dims(np.where(classifier_tm(X) == clas, 1, 0), axis=1)\n",
    "                class_sum += aM * I\n",
    "                \n",
    "            predictions = np.append(predictions, class_sum, axis=1)\n",
    "        \n",
    "        predictions = predictions[:,1:]\n",
    "\n",
    "        index_predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "        classes_array = np.array(classes)\n",
    "        y_pred = classes_array[index_predictions]\n",
    "        \n",
    "        return y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtain the train,test split and test for optimal M value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folder_name='image_dataset'\n",
    "\n",
    "(X_data, Y_data) = obtain_dataset(train_folder_name)\n",
    "\n",
    "# Shuffling to prevent bias\n",
    "randomize = np.arange(len(X_data))\n",
    "np.random.shuffle(randomize)\n",
    "\n",
    "X_data = X_data[randomize]\n",
    "Y_data = Y_data[randomize]\n",
    "\n",
    "k_fold = KFold(n_splits=5)\n",
    "classifiers_list = [i for i in range(5, 155, 5)]\n",
    "\n",
    "num_class_accuracy = []\n",
    "\n",
    "for num_classifiers in classifiers_list:\n",
    "\n",
    "    acc_scores = []\n",
    "    \n",
    "    for train_indices, test_indices in k_fold.split(X_data):\n",
    "  \n",
    "        bc = BoostingClassifier(num_classifiers)\n",
    "        bc.fit(X_data[train_indices], Y_data[train_indices])\n",
    "        y_pred = bc.predict(X_data[test_indices])\n",
    "        accuracy = accuracy_score(Y_data[test_indices], y_pred)\n",
    "        acc_scores.append(accuracy)\n",
    "    \n",
    "    num_class_accuracy.append(np.mean(acc_scores))\n",
    "    \n",
    "    print(num_classifiers, '> AVG accuracy: ', round(np.mean(acc_scores), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the boosting classifer and evaluate it on the train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple learning curve\n",
    "print(max(num_class_accuracy))\n",
    "plt.plot(classifiers_list, num_class_accuracy)\n",
    "plt.axhline(max(num_class_accuracy), color='k', alpha=0.5, linestyle='--')\n",
    "plt.xlabel(\"Number of Classifiers (M)\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folder_name='image_dataset'\n",
    "\n",
    "(X_data, Y_data) = obtain_dataset(train_folder_name)\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X_data, Y_data, train_size=0.8)\n",
    "\n",
    "bc = BoostingClassifier(155)\n",
    "bc.fit(Xtrain, ytrain)\n",
    "y_pred = bc.predict(Xtest)\n",
    "accuracy = accuracy_score(ytest, y_pred)\n",
    "print('> accuracy', round(accuracy, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test function that will be called to evaluate your code. Separate train and test dataset will be provided\n",
    "\n",
    "Do not modify the code below. Please write your code above such that it can be evaluated by the function below. You can modify your code above such that you obtain the best performance through this function. We will also be evaluating the cross-validation performance with a set train and val split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_func_boosting_image(image_dataset_train, image_dataset_test):\n",
    "    (X_train, Y_train) = obtain_dataset(image_dataset_train)\n",
    "    (X_test, Y_test) = obtain_dataset(image_dataset_test)\n",
    "    bc = BoostingClassifier()\n",
    "    bc.fit(X_train, Y_train)\n",
    "    y_pred = bc.predict(X_test)\n",
    "    print('accuracy', accuracy_score(Y_test, y_pred))\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2\n",
    "\n",
    "In this task, you need to classify the above dataset using a Support Vector Machine (SVM).\n",
    "\n",
    "This task is worth 25 points out of 100 points. You are allowed to use existing library functions such as scikit-learn for obtaining the SVM. The main idea is to analyse the dataset using different kind of kernels. You are also supposed to write your own custom kernels. The marking will be 15 marks for analysing the dataset using various kernels including your own kernels, 5 points for the performance on the test dataset and 5 points for a lab-report that provides the analysis and comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVMClassifier:\n",
    " \n",
    "    def __init__(self, kernel):\n",
    "        self.image_classifier = None\n",
    "        self.text_classifier = None\n",
    "        # Change the name of this parameter to test the other kenrnels\n",
    "        # ['linear', 'poly', 'rbf', 'sigmoid','laplacian', 'log', 'cauchy']\n",
    "        self.image_kernel = kernel\n",
    "        self.text_kernel = kernel\n",
    "#         self.image_kernel = 'laplacian'\n",
    "#         self.text_kernel = 'log'\n",
    "\n",
    "        self.image_X_train = None\n",
    "        self.text_X_train = None\n",
    "\n",
    "    def fit_image(self, X, y):\n",
    "        #training of the SVM \n",
    "        # providing for separate image kernels \n",
    "        \n",
    "        if self.image_kernel in ['linear', 'poly', 'rbf', 'sigmoid']:\n",
    "            clf = svm.SVC(kernel=self.image_kernel)\n",
    "            clf.fit(X, y)\n",
    "        else:\n",
    "            clf = svm.SVC(kernel='precomputed')\n",
    "            \n",
    "            if self.image_kernel == 'laplacian':\n",
    "                kernel_train = self.laplacian_kernel(X, X)\n",
    "                \n",
    "            elif self.image_kernel == 'log':\n",
    "                kernel_train = self.log_kernel(X, X)\n",
    "\n",
    "            elif self.image_kernel == 'cauchy':\n",
    "                kernel_train = self.cauchy_kernel(X, X)\n",
    "                        \n",
    "            clf.fit(kernel_train, y)\n",
    "            self.image_X_train = X\n",
    "        \n",
    "        self.image_classifier = clf\n",
    "    \n",
    "    def fit_text(self, X,y):\n",
    "        # training of the SVM\n",
    "\n",
    "        if self.text_kernel in ['linear', 'poly', 'rbf', 'sigmoid']:\n",
    "            clf = svm.SVC(kernel=self.text_kernel)\n",
    "            clf.fit(X, y)\n",
    "        else:\n",
    "            clf = svm.SVC(kernel='precomputed')\n",
    "            \n",
    "            if self.text_kernel == 'laplacian':\n",
    "                kernel_train = self.laplacian_kernel(X, X)\n",
    "                \n",
    "            elif self.text_kernel == 'log':\n",
    "                kernel_train = self.log_kernel(X, X)\n",
    "\n",
    "            elif self.text_kernel == 'cauchy':\n",
    "                kernel_train = self.cauchy_kernel(X, X)\n",
    "                        \n",
    "            clf.fit(kernel_train, y)\n",
    "            self.text_X_train = X\n",
    "        \n",
    "        self.text_classifier = clf\n",
    "    \n",
    "    \n",
    "    def predict_image(self, X):\n",
    "        # prediction routine for the SVM\n",
    "        \n",
    "        clf = self.image_classifier\n",
    "        \n",
    "        if self.image_kernel in ['linear', 'poly', 'rbf', 'sigmoid']:\n",
    "            y_pred = clf.predict(X)\n",
    "            \n",
    "        else:\n",
    "            X_train = self.image_X_train\n",
    "            \n",
    "            if self.image_kernel == 'laplacian':\n",
    "                kernel_test = self.laplacian_kernel(X, X_train)\n",
    "                \n",
    "            elif self.image_kernel == 'log':\n",
    "                kernel_test = self.log_kernel(X, X_train)\n",
    "\n",
    "            elif self.image_kernel == 'cauchy':\n",
    "                kernel_test = self.cauchy_kernel(X, X_train)\n",
    "        \n",
    "            y_pred = clf.predict(kernel_test)\n",
    "        \n",
    "        return y_pred\n",
    "        \n",
    "\n",
    "    def predict_text(self, X):\n",
    "        # prediction routine for the SVM\n",
    "        clf = self.text_classifier\n",
    "        \n",
    "        if self.text_kernel in ['linear', 'poly', 'rbf', 'sigmoid']:\n",
    "            y_pred = clf.predict(X)\n",
    "            \n",
    "        else:\n",
    "            X_train = self.text_X_train\n",
    "            \n",
    "            if self.text_kernel == 'laplacian':\n",
    "                kernel_test = self.laplacian_kernel(X, X_train)\n",
    "                \n",
    "            elif self.text_kernel == 'log':\n",
    "                kernel_test = self.log_kernel(X, X_train)\n",
    "\n",
    "            elif self.text_kernel == 'cauchy':\n",
    "                kernel_test = self.cauchy_kernel(X, X_train)\n",
    "        \n",
    "            y_pred = clf.predict(kernel_test)\n",
    "        \n",
    "        return y_pred\n",
    "    \n",
    "    def laplacian_kernel(self, X, y):\n",
    "        import numpy as np\n",
    "        from sklearn.metrics.pairwise import euclidean_distances\n",
    "        import scipy\n",
    "        \n",
    "        if scipy.sparse.issparse(X):\n",
    "            sigma = np.sqrt(X.shape[1] * np.var(X.toarray()))\n",
    "        else:\n",
    "            sigma = np.sqrt(X.shape[1] * np.var(X))\n",
    "\n",
    "        euclid_dist = euclidean_distances(X, y)\n",
    "        result = (- euclid_dist/ sigma)\n",
    "\n",
    "        return result\n",
    "    \n",
    "    \n",
    "    def log_kernel(self, X, y):\n",
    "        import numpy as np\n",
    "        from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "        d = 1\n",
    "        euclid_dist = euclidean_distances(X, y)\n",
    "        result = -np.log(euclid_dist**d + 1)\n",
    "        return result\n",
    "\n",
    "    \n",
    "    def cauchy_kernel(self, X, y):\n",
    "        import numpy as np\n",
    "        from sklearn.metrics.pairwise import euclidean_distances\n",
    "        import scipy\n",
    "\n",
    "        if scipy.sparse.issparse(X):\n",
    "            sigma = X.shape[1] * np.var(X.toarray())\n",
    "        else:\n",
    "            sigma = X.shape[1] * np.var(X)\n",
    "            \n",
    "        euclid_dist = euclidean_distances(X, y)\n",
    "\n",
    "        result = 1 / (1+ (euclid_dist**2 / sigma**2))\n",
    "        return result\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the SVM classifer and evaluate it on the train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel: linear | Accuracy:  0.5\n",
      "Kernel: poly | Accuracy:  0.49\n",
      "Kernel: rbf | Accuracy:  0.47\n",
      "Kernel: sigmoid | Accuracy:  0.21\n",
      "Kernel: laplacian | Accuracy:  0.5\n",
      "Kernel: log | Accuracy:  0.47\n",
      "Kernel: cauchy | Accuracy:  0.23\n",
      "Kernel: linear | Accuracy:  0.51\n",
      "Kernel: poly | Accuracy:  0.51\n",
      "Kernel: rbf | Accuracy:  0.49\n",
      "Kernel: sigmoid | Accuracy:  0.19\n",
      "Kernel: laplacian | Accuracy:  0.49\n",
      "Kernel: log | Accuracy:  0.51\n",
      "Kernel: cauchy | Accuracy:  0.19\n",
      "Kernel: linear | Accuracy:  0.51\n",
      "Kernel: poly | Accuracy:  0.51\n",
      "Kernel: rbf | Accuracy:  0.5\n",
      "Kernel: sigmoid | Accuracy:  0.25\n",
      "Kernel: laplacian | Accuracy:  0.51\n",
      "Kernel: log | Accuracy:  0.5\n",
      "Kernel: cauchy | Accuracy:  0.24\n",
      "Kernel: linear | Accuracy:  0.47\n",
      "Kernel: poly | Accuracy:  0.46\n",
      "Kernel: rbf | Accuracy:  0.47\n",
      "Kernel: sigmoid | Accuracy:  0.25\n",
      "Kernel: laplacian | Accuracy:  0.51\n",
      "Kernel: log | Accuracy:  0.51\n",
      "Kernel: cauchy | Accuracy:  0.27\n",
      "Kernel: linear | Accuracy:  0.51\n",
      "Kernel: poly | Accuracy:  0.5\n",
      "Kernel: rbf | Accuracy:  0.49\n",
      "Kernel: sigmoid | Accuracy:  0.24\n",
      "Kernel: laplacian | Accuracy:  0.52\n",
      "Kernel: log | Accuracy:  0.45\n",
      "Kernel: cauchy | Accuracy:  0.24\n",
      "Kernel: linear | Accuracy:  0.47\n",
      "Kernel: poly | Accuracy:  0.49\n",
      "Kernel: rbf | Accuracy:  0.47\n",
      "Kernel: sigmoid | Accuracy:  0.22\n",
      "Kernel: laplacian | Accuracy:  0.49\n",
      "Kernel: log | Accuracy:  0.47\n",
      "Kernel: cauchy | Accuracy:  0.25\n",
      "Kernel: linear | Accuracy:  0.49\n",
      "Kernel: poly | Accuracy:  0.47\n",
      "Kernel: rbf | Accuracy:  0.45\n",
      "Kernel: sigmoid | Accuracy:  0.21\n",
      "Kernel: laplacian | Accuracy:  0.51\n",
      "Kernel: log | Accuracy:  0.49\n",
      "Kernel: cauchy | Accuracy:  0.22\n",
      "Kernel: linear | Accuracy:  0.49\n",
      "Kernel: poly | Accuracy:  0.47\n",
      "Kernel: rbf | Accuracy:  0.48\n",
      "Kernel: sigmoid | Accuracy:  0.27\n",
      "Kernel: laplacian | Accuracy:  0.48\n",
      "Kernel: log | Accuracy:  0.47\n",
      "Kernel: cauchy | Accuracy:  0.24\n",
      "Kernel: linear | Accuracy:  0.47\n",
      "Kernel: poly | Accuracy:  0.49\n",
      "Kernel: rbf | Accuracy:  0.49\n",
      "Kernel: sigmoid | Accuracy:  0.22\n",
      "Kernel: laplacian | Accuracy:  0.51\n",
      "Kernel: log | Accuracy:  0.49\n",
      "Kernel: cauchy | Accuracy:  0.24\n",
      "Kernel: linear | Accuracy:  0.53\n",
      "Kernel: poly | Accuracy:  0.51\n",
      "Kernel: rbf | Accuracy:  0.53\n",
      "Kernel: sigmoid | Accuracy:  0.23\n",
      "Kernel: laplacian | Accuracy:  0.5\n",
      "Kernel: log | Accuracy:  0.48\n",
      "Kernel: cauchy | Accuracy:  0.24\n"
     ]
    }
   ],
   "source": [
    "image_kernels = ['linear', 'poly', 'rbf', 'sigmoid', 'laplacian', 'log', 'cauchy']\n",
    "kernels_df = pd.DataFrame([], columns=image_kernels)\n",
    "\n",
    "for _ in range(10):\n",
    "\n",
    "    train_folder_name='image_dataset'\n",
    "\n",
    "    (X_data, Y_data) = obtain_dataset(train_folder_name)\n",
    "\n",
    "    # Shuffling to prevent bias\n",
    "    randomize = np.arange(len(X_data))\n",
    "    np.random.shuffle(randomize)\n",
    "\n",
    "    X_data = X_data[randomize]\n",
    "    Y_data = Y_data[randomize]\n",
    "\n",
    "    k_fold = KFold(n_splits=10)\n",
    "    \n",
    "    kernel_accuracy = []\n",
    "\n",
    "    for kernel in image_kernels:\n",
    "\n",
    "        acc_scores = []\n",
    "\n",
    "        for train_indices, test_indices in k_fold.split(X_data):\n",
    "\n",
    "            sc = SVMClassifier(kernel)\n",
    "            sc.fit_image(X_data[train_indices], Y_data[train_indices])\n",
    "            y_pred = sc.predict_image(X_data[test_indices])\n",
    "            accuracy = accuracy_score(Y_data[test_indices], y_pred)\n",
    "            acc_scores.append(accuracy)\n",
    "\n",
    "        kernel_accuracy.append(np.mean(acc_scores))\n",
    "\n",
    "        print('Kernel:', kernel, '| Accuracy: ', round(np.mean(acc_scores), 2))\n",
    "        \n",
    "    df_temp = pd.DataFrame([kernel_accuracy], columns=image_kernels)\n",
    "    kernels_df = kernels_df.append(df_temp, ignore_index = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear       0.497333\n",
      "poly         0.490000\n",
      "rbf          0.484667\n",
      "sigmoid      0.228000\n",
      "laplacian    0.502000\n",
      "log          0.484667\n",
      "cauchy       0.236000\n",
      "dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Text(0.5, 0, ''), Text(0, 0.5, 'Accuracy')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "print(kernels_df.mean())\n",
    "kernelplot = sns.barplot(data=kernels_df)\n",
    "kernelplot.set(xlabel='', ylabel='Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SVMClassifier('laplacian')\n",
    "sc.fit_image(Xtrain, ytrain)\n",
    "y_pred = sc.predict_image(Xtest)\n",
    "print('accuracy', accuracy_score(ytest, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test function that will be called to evaluate your code. Separate train and test dataset will be provided\n",
    "\n",
    "Do not modify the code below. Please write your code above such that it can be evaluated by the function below. You can modify your code above such that you obtain the best performance through this function. We will also be evaluating the cross-validation performance with a set train and val split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_func_svm_image(image_dataset_train, image_dataset_test):\n",
    "    (X_train, Y_train) = obtain_dataset(image_dataset_train)\n",
    "    (X_test, Y_test) = obtain_dataset(image_dataset_test)\n",
    "    sc = SVMClassifier()\n",
    "    sc.fit_image(X_train, Y_train)\n",
    "    y_pred = sc.predict_image(X_test)\n",
    "    print('accuracy', accuracy_score(Y_test, y_pred))\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3\n",
    "\n",
    "In this task, you need to obtain sentiment analysis for the provided dataset. The dataset consists of movie reviews with the sentiments being provided. The sentiments are either positive or negative. You need to train a boosting based classifier to obtain train and cross-validate on the dataset provided. The method will be evaluated against an external test set.\n",
    "\n",
    "This task is worth 25 points out of 100 points. 15 points will be for implementing the pre-processing and Bag of Words based feature extractor correctly and evaluating the boosting based classifier for the text features and validating it by cross-validation on the training set. 5 points are based on the evaluation carried out on a separate test dataset that will be done at the time of evaluation. Finally 5 points are reserved for analysis of this part of the task and presenting it well in a lab report.\n",
    "\n",
    "Use the movie_review_train.csv file provided with the assignment, and save it in the same directory as the Python notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process the text and obtain a bag of words-based features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_bag_of_words(train_file):\n",
    "    # Write your preprocessor to process the text\n",
    "    # Write your own bag of words feature extractor using nltk and scikit-learn\n",
    "    # return (X_train,y_train,X_test,y_test)\n",
    "    \n",
    "    # Process training data first and ensure the test data is not used while extracting bag of words feature vector\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    from nltk.tokenize import RegexpTokenizer\n",
    "    \n",
    "    train_data = pd.read_csv(train_file)\n",
    "    # Shuffling to remove bias\n",
    "    train_data = train_data.sample(frac=1)\n",
    "\n",
    "    x_train = train_data['review']\n",
    "    y_train = train_data['sentiment']\n",
    "    \n",
    "    token = RegexpTokenizer(r'[a-z]+')\n",
    "    count_vect = CountVectorizer(lowercase=True, stop_words='english', tokenizer=token.tokenize)\n",
    "    X_train = count_vect.fit_transform(x_train)\n",
    "    \n",
    "    return X_train, y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_bag_of_words_train_test(train_file, test_file):\n",
    "    # Write your preprocessor to process the text\n",
    "    # Write your own bag of words feature extractor using nltk and scikit-learn\n",
    "    # return (X_train,y_train,X_test,y_test)\n",
    "    \n",
    "    # Process training data first and ensure the test data is not used while extracting bag of words feature vector\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    from nltk.tokenize import RegexpTokenizer\n",
    "    \n",
    "    train_data = pd.read_csv(train_file)\n",
    "    # Shuffling to remove bias\n",
    "    train_data = train_data.sample(frac=1)\n",
    "    \n",
    "    x_train = train_data['review']\n",
    "    y_train = train_data['sentiment']\n",
    "    \n",
    "    count_vect = CountVectorizer()\n",
    "    X_train = count_vect.fit_transform(x_train)\n",
    "\n",
    "    # Process testing data here. Ensure that test data is not used above\n",
    "    test_data = pd.read_csv(test_file)\n",
    "\n",
    "    x_test_data = test_data['review']\n",
    "    y_test = test_data['sentiment']\n",
    "    \n",
    "    X_test = count_vect.transform(x_test_data)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 38198) (1000, 38198) (4000,) (1000,)\n"
     ]
    }
   ],
   "source": [
    "train_file_name='movie_review_train.csv'\n",
    "\n",
    "(X_train, Y_train) = extract_bag_of_words(train_file_name)\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X_train, Y_train, train_size=0.8, random_state=1)\n",
    "print(Xtrain.shape, Xtest.shape, ytrain.shape, ytest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> accuracy 0.803\n"
     ]
    }
   ],
   "source": [
    "bc = BoostingClassifier(350)\n",
    "bc.fit(Xtrain, ytrain)\n",
    "y_pred = bc.predict(Xtest)\n",
    "accuracy = accuracy_score(ytest, y_pred)\n",
    "print('> accuracy', round(accuracy, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_class_cols = ['num_classifier', 'acccuracies']\n",
    "# num_class_df = pd.DataFrame([], columns=num_class_cols)\n",
    "\n",
    "# classifiers_list = [i for i in range(2, 121, 2)]\n",
    "classifiers_list = [150, 175, 200, 250, 300, 350, 400]\n",
    "\n",
    "for num_classifiers in classifiers_list:\n",
    "        \n",
    "    for _ in range(10):\n",
    "        \n",
    "        train_file_name='movie_review_train.csv'\n",
    "        (X_train, Y_train) = extract_bag_of_words(train_file_name)\n",
    "        Xtrain, Xtest, ytrain, ytest = train_test_split(X_train, Y_train, train_size=0.8, random_state=1)\n",
    "        \n",
    "        df_scores = [num_classifiers]\n",
    "        \n",
    "        bc = BoostingClassifier(num_classifiers)\n",
    "        bc.fit(Xtrain, ytrain)\n",
    "        y_pred = bc.predict(Xtest)\n",
    "        accuracy = accuracy_score(ytest, y_pred)\n",
    "        df_scores.append(accuracy)\n",
    "        \n",
    "        df_temp = pd.DataFrame([df_scores], columns=num_class_cols)\n",
    "        num_class_df = num_class_df.append(df_temp, ignore_index = True)    \n",
    "\n",
    "    print(num_classifiers, '> AVG accuracy: ', round(num_class_df[num_class_df['num_classifier']==num_classifiers].mean().values[1], 3))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.lineplot(data=num_class_df, x='num_classifier', y='acccuracies')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test function that will be called to evaluate your code. Separate train and test dataset will be provided\n",
    "\n",
    "Do not modify the code below. Please write your code above such that it can be evaluated by the function below. You can modify your code above such that you obtain the best performance through this function. We will also be evaluating the cross-validation performance with a set train and val split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_func_boosting_text(text_dataset_train, text_dataset_test):\n",
    "    (X_train, Y_train) = extract_bag_of_words(text_dataset_train)\n",
    "    (X_test, Y_test) = extract_bag_of_words(text_dataset_test)\n",
    "    bc = BoostingClassifier()\n",
    "    bc.fit(X_train, Y_train)\n",
    "    y_pred = bc.predict(X_test)    \n",
    "    print('accuracy', accuracy_score(Y_test, y_pred))\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4\n",
    "\n",
    "In this task, you need to classify the above movie review dataset using a Support Vector Machine (SVM).\n",
    "\n",
    "This task is worth 20 points out of 100 points. You are allowed to use existing library functions such as scikit-learn for obtaining the SVM. The main idea is to analyse the dataset using different kind of kernels. You are also supposed to write your own custom text kernels. The marking will be 10 marks for analysing the dataset using various kernels including your own kernels, 5 points for the performance on the test dataset and 5 points for a lab-report that provides the analysis and comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_kernels = ['linear', 'poly', 'rbf', 'sigmoid', 'laplacian', 'log', 'cauchy']\n",
    "text_df_cols = ['kernel', 'accuracy']\n",
    "\n",
    "text_kernels_df = pd.DataFrame([], columns=text_df_cols)\n",
    "\n",
    "for kernel in text_kernels:\n",
    "        \n",
    "    for _ in range(15):\n",
    "        \n",
    "        train_file_name='movie_review_train.csv'\n",
    "        (X_train, Y_train) = extract_bag_of_words(train_file_name)\n",
    "        Xtrain, Xtest, ytrain, ytest = train_test_split(X_train, Y_train, train_size=0.8, random_state=1)\n",
    "        \n",
    "        df_scores = [kernel]\n",
    "        \n",
    "        sc = SVMClassifier(kernel)\n",
    "        sc.fit_text(Xtrain, ytrain)\n",
    "        y_pred = sc.predict_text(Xtest)\n",
    "        accuracy = accuracy_score(ytest, y_pred)\n",
    "        df_scores.append(accuracy)\n",
    "        \n",
    "        df_temp = pd.DataFrame([df_scores], columns=text_df_cols)\n",
    "        text_kernels_df = text_kernels_df.append(df_temp, ignore_index = True)    \n",
    "        \n",
    "    print(kernel, '> AVG accuracy: ', round(text_kernels_df[text_kernels_df['kernel']==kernel].mean().values[0], 3))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "print(text_kernels_df.groupby('kernel')['accuracy'].mean())\n",
    "kernel_text_plot = sns.barplot(data=text_kernels_df, x='kernel', y='accuracy')\n",
    "kernel_text_plot.set(xlabel='', ylabel='Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code for svm based training of the dataset\n",
    "# ['linear', 'poly', 'rbf', 'sigmoid', 'laplacian', 'log', 'cauchy']\n",
    "train_file_name='movie_review_train.csv'\n",
    "\n",
    "(X_train, Y_train) = extract_bag_of_words(train_file_name)\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X_train, Y_train, train_size=0.8, random_state=1)\n",
    "print(Xtrain.shape, Xtest.shape, ytrain.shape, ytest.shape)\n",
    "\n",
    "sc = SVMClassifier('log')\n",
    "sc.fit_text(Xtrain, ytrain)\n",
    "y_pred = sc.predict_text(Xtest)\n",
    "print('accuracy', accuracy_score(ytest, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test function that will be called to evaluate your code. Separate train and test dataset will be provided\n",
    "\n",
    "Do not modify the code below. Please write your code above such that it can be evaluated by the function below. You can modify your code above such that you obtain the best performance through this function. We will also be evaluating the cross-validation performance with a set train and val split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_func_svm_text(text_dataset_train, text_dataset_test):\n",
    "    (X_train, Y_train) = extract_bag_of_words(text_dataset_train)\n",
    "    (X_test, Y_test) = extract_bag_of_words(text_dataset_test)\n",
    "    sc = SVMClassifier()\n",
    "    sc.fit_text(X_train, Y_train)\n",
    "    y_pred = sc.predict_text(X_test)\n",
    "    print('accuracy', accuracy_score(Y_test, y_pred))\n",
    "    return y_pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
