{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coursework 1 - revised and can be used with automarker\n",
    "\n",
    "In this coursework you will be aiming to complete two classification tasks. One of the classification tasks is related to image classification and the other relates to text classification.\n",
    "\n",
    "The specific tasks and the marking for the various tasks are provided in the notebook. Each task is expected to be accompanied by a lab-report. Each task can have a concise lab report that is maximum of one page in an A4 size. You will be expected to submit your Jupyter Notebook and all lab reports as a single PDF file. You could have additional functions implemented that you require for carrying out each task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1\n",
    "\n",
    "In this task, you are provided with three classes of images, cars, bikes and people in real world settings. You are provided with code for obtaining features for these images (specifically histogram of gradients (HoG) features). You need to implement a boosting based classifier that can be used to classify the images. \n",
    "\n",
    "This task is worth 30 points out of 100 points. \n",
    "Implementing a working boosting based classifier and validating it by cross-validation on the training set will be evaluated for 15 out of 30 points. 10 points are based on the evaluation carried out on a separate test dataset that will be done at the time of evaluation. Finally 5 points are reserved for analysis of this part of the task and presenting it well in a lab report. \n",
    "\n",
    "Note that the boosting classifier you implement can include decision trees from your previous ML1 coursework or can be a decision stump. Use the image_dataset directory provided with the assignment and save it in the same directory as the Python notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write your  Image feature extraction code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_dataset(folder_name):\n",
    "    import cv2\n",
    "    import numpy as np\n",
    "    import glob\n",
    "\n",
    "    # assuming 128x128 size images and HoGDescriptor length of 34020\n",
    "    hog_feature_len=34020\n",
    "    hog = cv2.HOGDescriptor()\n",
    "    \n",
    "    X = np.empty([0,hog_feature_len], dtype=float)\n",
    "    ytargets = []\n",
    "    \n",
    "    directories_list = ['bikes', 'people', 'cars']\n",
    "    for directory in directories_list:\n",
    "        for filename in glob.glob(folder_name+'/'+directory+'/*.png'):\n",
    "            \n",
    "            im = cv2.imread(filename)\n",
    "            h = hog.compute(im).reshape(1,-1)\n",
    "\n",
    "            X = np.append(X, h, axis=0)\n",
    "            ytargets.append(directory)\n",
    "\n",
    "    y = np.array(ytargets)\n",
    "\n",
    "    return (X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Optional function for those who want to include pre-processing for train data in obtain dataset\n",
    "# def obtain_dataset_train_test(folder_name_train, folder_name_test):\n",
    "#     import cv2\n",
    "#     import numpy as np    \n",
    "#     # assuming 128x128 size images and HoGDescriptor length of 34020\n",
    "#     hog_feature_len=34020\n",
    "#     hog = cv2.HOGDescriptor()\n",
    "#     #code for obtaining hog feature for one image file name\n",
    "#     # im = cv2.imread(image_filename)\n",
    "#     # h = hog.compute(image)\n",
    "#     # use this to read all images in the three directories and obtain the set of features X and train labels Y\n",
    "#     # you can assume there are three different classes in the image dataset\n",
    "#     #Process train images first, do NOT include test images in this stage\n",
    "#     # Process test images over here while ensuring that there is no data leakage from train to test\n",
    "#     # For instance dimensionality reduction should not be computed over train and test images jointly\n",
    "#     return (X_train, y_train, X_test, y_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boosting classifier class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoostingClassifier:\n",
    "    # You need to implement this classifier. \n",
    "    def __init__(self):\n",
    "        #implement initialisation\n",
    "        self.num_classifiers = None\n",
    "        self.classifiers = []\n",
    "        self.classes = []\n",
    "    \n",
    "    def fit(self, X,y):\n",
    "        #implement training of the boosting classifier\n",
    "        import numpy as np\n",
    "        from sklearn.tree import DecisionTreeClassifier\n",
    "        \n",
    "        n_samples, n_features = X.shape\n",
    "        self.classes = np.unique(y)\n",
    "        \n",
    "        # number of classifiers for images\n",
    "        if len(self.classes) == 3:\n",
    "            \n",
    "            # You can change this value to impact the value of \n",
    "            # M for the image dataset\n",
    "            self.num_classifiers = 155\n",
    "            \n",
    "        # number of classifiers for text\n",
    "        elif len(self.classes) == 2:\n",
    "            \n",
    "            # You can change this value to impact the value of \n",
    "            # M for the text dataset\n",
    "            self.num_classifiers = 350\n",
    "        \n",
    "        # Initialize weights to 1/N\n",
    "        weights = np.full(n_samples, (1 / n_samples), dtype=np.float32)\n",
    "        \n",
    "        for classifier_index in range(self.num_classifiers):\n",
    "            classifier_tm = DecisionTreeClassifier(max_depth=1).fit(X,y,sample_weight=weights).predict\n",
    "            \n",
    "            I = np.where(classifier_tm(X) != y, 1, 0)\n",
    "            errM = sum(weights * I) / sum(weights)\n",
    "            \n",
    "            aM = np.log((1-errM)/errM) + np.log(len(self.classes)-1)\n",
    "        \n",
    "            for i in range(weights.shape[0]):\n",
    "                weights[i] = weights[i]*np.exp(aM*I[i])\n",
    "        \n",
    "            self.classifiers.append((aM, classifier_tm))\n",
    "        \n",
    "    \n",
    "    def predict(self, X):\n",
    "        # implement prediction of the boosting classifier\n",
    "        import numpy as np\n",
    "        from sklearn.tree import DecisionTreeClassifier\n",
    "        \n",
    "        classes = self.classes\n",
    "        \n",
    "        predictions = np.zeros([X.shape[0],1])\n",
    "        # Through each class\n",
    "        for clas in classes:\n",
    "            \n",
    "            class_sum = np.zeros([X.shape[0], 1])\n",
    "            \n",
    "            # Through each classifier\n",
    "            for classifier_index in range(self.num_classifiers):\n",
    "                \n",
    "                aM, classifier_tm = self.classifiers[classifier_index]\n",
    "\n",
    "                # To update sum\n",
    "                I = np.expand_dims(np.where(classifier_tm(X) == clas, 1, 0), axis=1)\n",
    "                class_sum += aM * I\n",
    "                \n",
    "            predictions = np.append(predictions, class_sum, axis=1)\n",
    "        \n",
    "        predictions = predictions[:,1:]\n",
    "\n",
    "        index_predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "        classes_array = np.array(classes)\n",
    "        y_pred = classes_array[index_predictions]\n",
    "        \n",
    "        return y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test function that will be called to evaluate your code. Separate train and test dataset will be provided\n",
    "\n",
    "Do not modify the code below. Please write your code above such that it can be evaluated by the function below. You can modify your code above such that you obtain the best performance through this function. We will also be evaluating the cross-validation performance with a set train and val split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_func_boosting_image(image_dataset_train, image_dataset_test):\n",
    "    import numpy as np\n",
    "    import cv2\n",
    "    import glob\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    \n",
    "    (X_train, Y_train) = obtain_dataset(image_dataset_train)\n",
    "    (X_test, Y_test) = obtain_dataset(image_dataset_test) # optionally replace the two calls with a single call to obtain_dataset_train_test() function\n",
    "    bc = BoostingClassifier()\n",
    "    bc.fit(X_train, Y_train)\n",
    "    y_pred = bc.predict(X_test)\n",
    "    acc = accuracy_score(Y_test, y_pred)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2\n",
    "\n",
    "In this task, you need to classify the above dataset using a Support Vector Machine (SVM).\n",
    "\n",
    "This task is worth 25 points out of 100 points. You are allowed to use existing library functions such as scikit-learn for obtaining the SVM. The main idea is to analyse the dataset using different kind of kernels. You are also supposed to write your own custom kernels. The marking will be 15 marks for analysing the dataset using various kernels including your own kernels, 5 points for the performance on the test dataset and 5 points for a lab-report that provides the analysis and comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVMClassifier:\n",
    " \n",
    "    def __init__(self):\n",
    "        self.image_classifier = None\n",
    "        self.text_classifier = None\n",
    "        \n",
    "        # Change the name of these parameter to test the corresponding dataset\n",
    "        # On the other kenrnels. Choose from this list\n",
    "        # ['linear', 'poly', 'rbf', 'sigmoid','laplacian', 'log', 'cauchy']\n",
    "        self.image_kernel = 'laplacian'\n",
    "        self.text_kernel = 'laplacian'\n",
    "\n",
    "        self.image_X_train = None\n",
    "        self.text_X_train = None\n",
    "\n",
    "    def fit_image(self, X, y):\n",
    "        #training of the SVM \n",
    "        # providing for separate image kernels \n",
    "        from sklearn import svm\n",
    "\n",
    "        if self.image_kernel in ['linear', 'poly', 'rbf', 'sigmoid']:\n",
    "            clf = svm.SVC(kernel=self.image_kernel)\n",
    "            clf.fit(X, y)\n",
    "        else:\n",
    "            clf = svm.SVC(kernel='precomputed')\n",
    "            \n",
    "            if self.image_kernel == 'laplacian':\n",
    "                kernel_train = self.laplacian_kernel(X, X)\n",
    "                \n",
    "            elif self.image_kernel == 'log':\n",
    "                kernel_train = self.log_kernel(X, X)\n",
    "\n",
    "            elif self.image_kernel == 'cauchy':\n",
    "                kernel_train = self.cauchy_kernel(X, X)\n",
    "                        \n",
    "            clf.fit(kernel_train, y)\n",
    "            self.image_X_train = X\n",
    "        \n",
    "        self.image_classifier = clf\n",
    "    \n",
    "    def fit_text(self, X,y):\n",
    "        # training of the SVM\n",
    "        from sklearn import svm\n",
    "\n",
    "        if self.text_kernel in ['linear', 'poly', 'rbf', 'sigmoid']:\n",
    "            clf = svm.SVC(kernel=self.text_kernel)\n",
    "            clf.fit(X, y)\n",
    "        else:\n",
    "            clf = svm.SVC(kernel='precomputed')\n",
    "            \n",
    "            if self.text_kernel == 'laplacian':\n",
    "                kernel_train = self.laplacian_kernel(X, X)\n",
    "                \n",
    "            elif self.text_kernel == 'log':\n",
    "                kernel_train = self.log_kernel(X, X)\n",
    "\n",
    "            elif self.text_kernel == 'cauchy':\n",
    "                kernel_train = self.cauchy_kernel(X, X)\n",
    "                        \n",
    "            clf.fit(kernel_train, y)\n",
    "            self.text_X_train = X\n",
    "        \n",
    "        self.text_classifier = clf\n",
    "    \n",
    "    \n",
    "    def predict_image(self, X):\n",
    "        # prediction routine for the SVM\n",
    "        from sklearn import svm\n",
    "\n",
    "        clf = self.image_classifier\n",
    "        \n",
    "        if self.image_kernel in ['linear', 'poly', 'rbf', 'sigmoid']:\n",
    "            y_pred = clf.predict(X)\n",
    "            \n",
    "        else:\n",
    "            X_train = self.image_X_train\n",
    "            \n",
    "            if self.image_kernel == 'laplacian':\n",
    "                kernel_test = self.laplacian_kernel(X, X_train)\n",
    "                \n",
    "            elif self.image_kernel == 'log':\n",
    "                kernel_test = self.log_kernel(X, X_train)\n",
    "\n",
    "            elif self.image_kernel == 'cauchy':\n",
    "                kernel_test = self.cauchy_kernel(X, X_train)\n",
    "        \n",
    "            y_pred = clf.predict(kernel_test)\n",
    "        \n",
    "        return y_pred\n",
    "        \n",
    "\n",
    "    def predict_text(self, X):\n",
    "        # prediction routine for the SVM\n",
    "        from sklearn import svm\n",
    "\n",
    "        clf = self.text_classifier\n",
    "        \n",
    "        if self.text_kernel in ['linear', 'poly', 'rbf', 'sigmoid']:\n",
    "            y_pred = clf.predict(X)\n",
    "            \n",
    "        else:\n",
    "            X_train = self.text_X_train\n",
    "            \n",
    "            if self.text_kernel == 'laplacian':\n",
    "                kernel_test = self.laplacian_kernel(X, X_train)\n",
    "                \n",
    "            elif self.text_kernel == 'log':\n",
    "                kernel_test = self.log_kernel(X, X_train)\n",
    "\n",
    "            elif self.text_kernel == 'cauchy':\n",
    "                kernel_test = self.cauchy_kernel(X, X_train)\n",
    "        \n",
    "            y_pred = clf.predict(kernel_test)\n",
    "        \n",
    "        return y_pred\n",
    "    \n",
    "    def laplacian_kernel(self, X, y):\n",
    "        import numpy as np\n",
    "        from sklearn.metrics.pairwise import euclidean_distances\n",
    "        import scipy\n",
    "        \n",
    "        if scipy.sparse.issparse(X):\n",
    "            sigma = np.sqrt(X.shape[1] * np.var(X.toarray()))\n",
    "        else:\n",
    "            sigma = np.sqrt(X.shape[1] * np.var(X))\n",
    "\n",
    "        euclid_dist = euclidean_distances(X, y)\n",
    "        result = (- euclid_dist/ sigma)\n",
    "\n",
    "        return result\n",
    "    \n",
    "    \n",
    "    def log_kernel(self, X, y):\n",
    "        import numpy as np\n",
    "        from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "        d = 1\n",
    "        euclid_dist = euclidean_distances(X, y)\n",
    "        result = -np.log(euclid_dist**d + 1)\n",
    "        return result\n",
    "\n",
    "    \n",
    "    def cauchy_kernel(self, X, y):\n",
    "        import numpy as np\n",
    "        from sklearn.metrics.pairwise import euclidean_distances\n",
    "        import scipy\n",
    "\n",
    "        if scipy.sparse.issparse(X):\n",
    "            sigma = X.shape[1] * np.var(X.toarray())\n",
    "        else:\n",
    "            sigma = X.shape[1] * np.var(X)\n",
    "            \n",
    "        euclid_dist = euclidean_distances(X, y)\n",
    "\n",
    "        result = 1 / (1+ (euclid_dist**2 / sigma**2))\n",
    "        return result\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test function that will be called to evaluate your code. Separate train and test dataset will be provided\n",
    "\n",
    "Do not modify the code below. Please write your code above such that it can be evaluated by the function below. You can modify your code above such that you obtain the best performance through this function. We will also be evaluating the cross-validation performance with a set train and val split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_func_svm_image(image_dataset_train, image_dataset_test):\n",
    "    import numpy as np\n",
    "    import cv2\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    \n",
    "    (X_train, Y_train) = obtain_dataset(image_dataset_train)\n",
    "    (X_test, Y_test) = obtain_dataset(image_dataset_test) # optionally replace the two calls with a single call to obtain_dataset_train_test() function\n",
    "    sc = SVMClassifier()\n",
    "    sc.fit_image(X_train, Y_train)\n",
    "    y_pred = sc.predict_image(X_test)\n",
    "    acc = accuracy_score(Y_test, y_pred)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3\n",
    "\n",
    "In this task, you need to obtain sentiment analysis for the provided dataset. The dataset consists of movie reviews with the sentiments being provided. The sentiments are either positive or negative. You need to train a boosting based classifier to obtain train and cross-validate on the dataset provided. The method will be evaluated against an external test set.\n",
    "\n",
    "This task is worth 25 points out of 100 points. 15 points will be for implementing the pre-processing and Bag of Words based feature extractor correctly and evaluating the boosting based classifier for the text features and validating it by cross-validation on the training set. 5 points are based on the evaluation carried out on a separate test dataset that will be done at the time of evaluation. Finally 5 points are reserved for analysis of this part of the task and presenting it well in a lab report.\n",
    "\n",
    "Use the movie_review_train.csv file provided with the assignment, and save it in the same directory as the Python notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process the text and obtain a bag of words-based features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_bag_of_words(train_file):\n",
    "#     import numpy as np\n",
    "#     import pandas as pd\n",
    "#     from sklearn.feature_extraction.text import CountVectorizer\n",
    "#     from nltk.tokenize import RegexpTokenizer\n",
    "#     data = pd.read_csv(train_file)\n",
    "#     # Shuffling to remove bias\n",
    "#     data = data.sample(frac=1)\n",
    "#     data = data.sample(frac=1)\n",
    "#     x = data['review']\n",
    "#     y = data['sentiment']\n",
    "#     count_vect = CountVectorizer()\n",
    "#     X = count_vect.fit_transform(x)\n",
    "#     return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_bag_of_words_train_test(train_file, test_file):\n",
    "    # Write your preprocessor to process the text\n",
    "    # Write your own bag of words feature extractor using nltk and scikit-learn\n",
    "    # return (X_train,y_train,X_test,y_test)\n",
    "    \n",
    "    # Process training data first and ensure the test data is not used while extracting bag of words feature vector\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    from nltk.tokenize import RegexpTokenizer\n",
    "    \n",
    "    train_data = pd.read_csv(train_file)\n",
    "    # Shuffling to remove bias\n",
    "    train_data = train_data.sample(frac=1)\n",
    "    \n",
    "    x_train = train_data['review']\n",
    "    y_train = train_data['sentiment']\n",
    "    \n",
    "    token = RegexpTokenizer(r'[a-z]+')\n",
    "    count_vect = CountVectorizer(lowercase=True, stop_words='english', tokenizer=token.tokenize)\n",
    "    \n",
    "    X_train = count_vect.fit_transform(x_train)\n",
    "\n",
    "    # Process testing data here. Ensure that test data is not used above\n",
    "    test_data = pd.read_csv(test_file)\n",
    "\n",
    "    x_test_data = test_data['review']\n",
    "    y_test = test_data['sentiment']\n",
    "    \n",
    "    # Using tranform instead of fit transform to prevent leakage\n",
    "    X_test = count_vect.transform(x_test_data)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test function that will be called to evaluate your code. Separate train and test dataset will be provided\n",
    "\n",
    "Do not modify the code below. Please write your code above such that it can be evaluated by the function below. You can modify your code above such that you obtain the best performance through this function. We will also be evaluating the cross-validation performance with a set train and val split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_func_boosting_text(text_dataset_train, text_dataset_test):\n",
    "    import numpy as np\n",
    "    import cv2\n",
    "    import pandas as pd\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    from nltk.tokenize import RegexpTokenizer\n",
    "    \n",
    "    (X_train, Y_train, X_test, Y_test) = extract_bag_of_words_train_test(text_dataset_train, text_dataset_test)\n",
    "    bc = BoostingClassifier()\n",
    "    bc.fit(X_train, Y_train)\n",
    "    y_pred = bc.predict(X_test)    \n",
    "    acc = accuracy_score(Y_test, y_pred)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4\n",
    "\n",
    "In this task, you need to classify the above movie review dataset using a Support Vector Machine (SVM).\n",
    "\n",
    "This task is worth 20 points out of 100 points. You are allowed to use existing library functions such as scikit-learn for obtaining the SVM. The main idea is to analyse the dataset using different kind of kernels. You are also supposed to write your own custom text kernels. The marking will be 10 marks for analysing the dataset using various kernels including your own kernels, 5 points for the performance on the test dataset and 5 points for a lab-report that provides the analysis and comparisons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test function that will be called to evaluate your code. Separate train and test dataset will be provided\n",
    "\n",
    "Do not modify the code below. Please write your code above such that it can be evaluated by the function below. You can modify your code above such that you obtain the best performance through this function. We will also be evaluating the cross-validation performance with a set train and val split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_func_svm_text(text_dataset_train, text_dataset_test):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn import svm\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    from nltk.tokenize import RegexpTokenizer\n",
    "    \n",
    "    (X_train, Y_train, X_test, Y_test) = extract_bag_of_words_train_test(text_dataset_train, text_dataset_test)\n",
    "    sc = SVMClassifier()\n",
    "    sc.fit_text(X_train, Y_train)\n",
    "    y_pred = sc.predict_text(X_test)\n",
    "    acc = accuracy_score(Y_test, y_pred)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
